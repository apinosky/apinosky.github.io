---
layout: post
title: The Baxter Builder (ROS)
subtitle: ME 495 â€“ Embedded Systems in Robotics
date: 2019-12-21 06:49:15.000000000 -06:00
categories:
- GradProjects
permalink: "/2019/12/21/ROS/"
photo: "/assets/images/baxter.jpeg"
---

In the first quarter of my graduate program, I took a project-based course, which provided experience with a variety of software tools and concepts useful for a robotics engineer working with practical embedded systems. In this course, I learned how to use the Robot Operating System (ROS), with all major assignments programmed in Python. This page highlights the final project completed at the end of the course. The "theme" of the final project was *Recreational Robotics*.

# Goals of the Project
For this project, the robot tasks had to include:

1. _Manuipulation:_ The robot must physically interact with at least one object in the environment using the robot's arm and or grippers
2. _Sensing:_ The solution must use at least one sensor, and the behavior of the robot must change in response to the sensor reading
3. _Human Interaction:_ A human user should be able to do something to change what the robot does, without needing to program

# Final Task Overview
My group used a Baxter robot to assemble a MEGA BLOKS pyramid from the blocks provided by the user. The Baxter robot has two arms. For this task, one arm was used for visualization, and the other was used for manipulation. The general flow is:

1.    Initialize Baxter
1.    Human places blocks on tray &#8594; _Human Interaction_
1.    Baxter detects location of a block (with right hand) &#8594; _Sensing_
1.    Goal block is displayed to user on screen
1.    Baxter navigates to block, picks it up, and places it in the pyramid location (with left hand) &#8594; _Manipulation_
1.    Baxter repeats steps 3 and 4 until the pyramid is complete

# My Role
For this project, I set up the initialization protocol for the task. I also Luxi set up the MoveIt!, which was used to move the arm into place over a block prior to pickup. I served as systems the systems integrator for the project -- setting up the overall flow and ensuring the sensing and manipulation portions of the project were compatible. When we had to pivot from our initial plan of using pure position control to force control to manipulate the blocks, I worked with Luxi to make the necessary controls modifications.

# Video

<iframe width="669" height="459" src="https://www.youtube.com/embed/mz1FwBR94og" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

# Future Improvements

- Adding closed-loop control with the camera (to verify block placement and allow more accurate positioning)
- Updating feedback control on the gripper to sense if the gripper is indeed holding the block at the desired position

#### Team members: Allie Pinosky, Luxi Huang, Marcel Bonnici, Senthil Palanisamy
![team]( /assets/images/megablox.png)
